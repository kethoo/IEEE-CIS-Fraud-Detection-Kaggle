# IEEE-CIS Fraud Detection

## პროექტის შესახებ

ამ ქომფეთიშენის მიზანი იყო ელქტრონული გადახდის სისტემაში, არსებულ მონაცემებზე დაყრდნობით, გაგვეუმჯობესებინა და შეგვეძლო თაღლითური ტრანზაქციების ამოცნობა. ინფორმაცია 2 ძირითად ნაწილად იყო გაყოფილი: `train_transaction` და `train_identity`. პირველი შეიცავს მონაცამებს ტრანზაქციების შესახებ, მაგალითად: Id, DateTime, Amount..., ხოლო მეორე კი შეიცავს დამატებით ინფორმაციას, რომელიც გვეხმარება მომხმარებლის იდენტიფიკაციაში და უკავშირდება ტრანზაქციებს transactionID-ს საშუალებით.

ერთ-ერთი მთავარი დეტალი არის ის, რომ ჩვენი target feature - `isFraud` არის ძალიან არაბალანსირებული. ჩემი მიდგომა იყო შემდეგგვარი: მონაცემთა გაწმენდა და მომზადება ამ არადაბალანსებული კლასებისთვის, კატეგორიული ცვლადების ეფექტურობა და ენკოდირება, მაღალი კორელაციის მქონე მონაცემების გაფილტვრა, RFE საუკეთესო მახასიათებლების შესარჩევად, სხვადასხვა მოდელზე დატრეინება, შედარება, მოდელის ოპტიმიზაცია წონების გამოყენებით და შემდეგ MLflow-ს საშუალებით ექსპერიმენტებზე დაკვირვება და საუკეთესო მოდელის არჩევა.

თავდაპირველად, აღვნიშნავ, რომ ჩემი დეითა დავყავი 2 ნაწილად: numerical და categorical. ქონთესთის დეითაში აღწერილი იყო კატეგორიული ცვლადები, თუმცა მათგან რამდენიმე იყო int/float ტიპის და შესაბამისად, ისინი გადავაქციე სტრინგებად, რათა რეალურად კატეგორიული ცვლადები ყოფილიყო.

## რეპოზიტორიის სტრუქტურა

- **basis_model.ipynb** - სანამ უშუალოდ ამოცანის ამოხსნას გადავწყვეტდი, გადავწყვიტე ჩამომეწერა ყველა ის ფუნქცია, რომელიც შემდეგ უნდა დამეიმპლემენტირებინა თითოეული მოდელისათვის. შემდეგ ეგენი გადავთარგმნე კლასებად და გამოვიყენე მოდელებში. შეამჩვნევთ, რომ სუულ ყველაფერი არ მაქვს გამოყენებული, ლეპტოპი მომიკვდა და შესაბამისად არ გამოვიდა ≡(▔﹏▔)≡

- **model_experiment_XGBoost.ipynb** - XGBoost cleaning, feature engineering, feature selection, training

- **model_experiment_RandomForest.ipynb** - RandomForest cleaning, feature engineering, feature selection, training

- **model_experiment_LogicalRegression.ipynb** - LogicalRegression cleaning, feature engineering, feature selection, training
    
- **model_inference.ipynb** - ხდება ტესტ სეტზე პროგნოზი და საუკეთესო მოდელის გამოყენებით ვაგენერირებ საბმიშენს.

## Feature Engineering

### კატეგორიული ცვლადების რიცხვითში გადაყვანა
გამოვიყენე 2 მიდგომა:
- **ბინალური ცვლადებისთვის**: One-hot Encoding
- **მრავალმნიშვნელოვანი ცვლადებისთვის**: WOE Encoding, რომელიც საშუალებას გვაძლევდა გამოგვეთვალა ყოველი კატეგორიის სტატისტიკური მნიშვნელობა თაღლითობის პრედიქშენისათვის.

### Nan მნიშვნელობების დამუშავება
მონაცემების მიმოხილვისას გავიგე, რომ რაღაც ნაწილი შეიცავდა პროცენტულად ძალიან დიდ დაკარგულ მნიშვნელობებს, რაც ცხადია პრობლემას ქმნის დამუშავებისათვის. ამიტომაც, 3 ვარიანტად ჩავშალე ამის მოგვარება:
- ის სვეტები, რომელთა მონაცემთა 90% იყო დაკარგული, უბრალოდ მოვიშორე
- რიცხვითი ცვლადები შევავსე მედიანით, რადგან ნაკლებად მგრძნობიარეა ასე
- კატეგორიული ცვლადები კი შევავსე "NotAv" (Not Available)-ით, რაც საშუალებას გვაძლევს, რომ ცალკე კატეგორიად განიხილოს დაკარგული მონაცემები

## Cleaning

როგორც ვნახეთ, ძალიან დიდი დეითასეთი გვაქვს, ამიტომ ქლინინგს საკმაოდ დიდი მნიშვნელობა აქვს. იმისათვის, რომ გამეფილტრა მონაცემები, დავაკვირდი კორელაციას, დავანორმალიზირე.

### მაღალი კორელაცია
სვეტები, რომლებიც 85%-ზე მეტ კორელაციას ავლენენ ერთმანეთთან, იფილტრება.

### ნორმალიზაცია
რიცხვითი ცვლადების კატეგორიულში გადაყვანა, რომელიც ზემოთ აღვწერე დეტალურად.

## Feature Selection

კვლევის შედეგად, გადავწყვიტე გამომეყენებინა RFE და ორფაზიანი შერჩევა.

### RFE
რეკურსიული ნიშნების ელიმინაციით ეტაპობრივად ვამცირებთ feature-ების რაოდენობას ყველაზე ნაკლებად მნიშვნელოვანი ნიშნების გადაგდებით.

### ორფაზიანი შერჩევა
საშუალებას მაძლევს პირველ ფაზაში სწრაფად გავფილტრო აშკარად უმნიშვნელო ნიშნები, ხოლო მეორე ფაზაში კი უფრო ზუსტად ვარჩევთ ამ ნიშნებს.

## Training

### ტესტირებული მოდელები

#### XGBoost
- **უპირატესობები**: მაღალი სიზუსტე, კარგად მუშაობს არაბალანსირებულ მონაცემებზე
- **გამოწვევები**: მგრძნობიარეა ჰიპერპარამეტრების მიმართ
- **შედეგები**: AUC = 0.929, AP = 0.623, F1 = 0.345
    
#### Random Forest
- **უპირატესობები**: მდგრადია გადაჭარბებული მორგების მიმართ, ნაკლებად მგრძნობიარეა ჰიპერპარამეტრების მიმართ
- **გამოწვევები**: უფრო მეხსიერებატევადია დიდი მონაცემებისთვის
    
#### Logistic Regression
- **უპირატესობები**: სწრაფი, ინტერპრეტირებადი, მდგრადი
- **გამოწვევები**: ნაკლებად ზუსტი რთული დამოკიდებულებებისთვის

## MLflow Tracking

### დალოგილი მეტრიკები

- **validation_auc** - რეცეივერის ოპერაციული მახასიათებლის (ROC) მრუდის ქვეშ ფართობი

- **validation_ap** - საშუალო პრეციზია (Average Precision)
  - პრეციზია-გამოძავების მრუდის ქვეშ ფართობი
  - ზომავს მოდელის საშუალო პრეციზიას სხვადასხვა გამოძავების დონეზე
  - განსაკუთრებით სასარგებლოა არაბალანსირებული მონაცემებისთვის
  - უფრო მაღალი მნიშვნელობა ნიშნავს უკეთეს მოდელს

- **validation_f1** - F1 ქულა
  - პრეციზიისა და გამოძავების ჰარმონიული საშუალო
  - დაბალანსებული მეტრიკა მოდელის ეფექტურობის შესაფასებლად
  - 0 (ყველაზე ცუდი) - 1 (საუკეთესო) დიაპაზონში

- **fraud_precision** - პრეციზია თაღლითობის კლასისთვის
  - თაღლითურად კლასიფიცირებული ტრანზაქციებიდან რეალურად თაღლითურის პროცენტი
  - ზომავს, რამდენად ზუსტია მოდელი, როდესაც ის ტრანზაქციას თაღლითურად ახასიათებს
  - მაღალი პრეციზია ნიშნავს ნაკლებ ცრუ დადებით შედეგებს

- **fraud_recall** - recall თაღლითობის კლასისთვის
  - რეალურად თაღლითური ტრანზაქციებიდან რამდენი იქნა სწორად იდენტიფიცირებული მოდელის მიერ
  - ზომავს, რამდენად კარგად ახერხებს მოდელი ყველა თაღლითური შემთხვევის პოვნას
  - მაღალი გამოძავება ნიშნავს ნაკლებ ცრუ უარყოფით შედეგებს

## Best Model Results

| მეტრიკა | მნიშვნელობა |
|---------|-------------|
| validation_auc | 0.9271012002620643 |
| validation_ap | 0.6257440549430572 |
| validation_f1 | 0.46439533456108034 |
| fraud_precision | 0.3400254700726646 |
| fraud_recall | 0.732214873 |

**Competition score**: 0.904
